<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en lang=en-us><head><link href=http://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><title>A survey of robots.txt - part one &#183; Jamie Magee</title><link rel=preload href=/css/poole.css as=style><link rel=preload href=/css/hyde.css as=style><link rel=preload href=/css/poole-overrides.css as=style><link rel=preload href=/css/hyde-overrides.css as=style><link rel=preload href=/css/hyde-x.css as=style><link rel=preload href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface" as=font><link rel=preload href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css as=style><link rel=stylesheet href=/css/poole.css><link rel=stylesheet href=/css/hyde.css><link rel=stylesheet href=/css/poole-overrides.css><link rel=stylesheet href=/css/hyde-overrides.css><link rel=stylesheet href=/css/hyde-x.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface"><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css><style>@font-face{font-family:berkeley mono;src:url(/fonts/BerkeleyMono-Regular.woff2)format('woff2')}</style><link rel=stylesheet href=/css/jamie.css><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link href=/favicon.ico rel=icon><meta name=description content><meta name=keywords content><meta property="og:title" content="A survey of robots.txt - part one"><meta property="og:description" content="After reading CollinMorris&rsquo;s analysis of favicons of the top 1 million sites on the web, I thought it would be interesting to do the same for other common parts of websites that often get overlooked.
The robots.txt file is a plain text file found at on most websites which communicates information to web crawlers and spiders about how to scan a website.. For example, here&rsquo;s an excerpt from robots.txt for google."><meta property="og:type" content="article"><meta property="og:url" content="/blog/a-survey-of-robots-txt-part-one/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2017-09-19T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-19T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A survey of robots.txt - part one"><meta name=twitter:description content="After reading CollinMorris&rsquo;s analysis of favicons of the top 1 million sites on the web, I thought it would be interesting to do the same for other common parts of websites that often get overlooked.
The robots.txt file is a plain text file found at on most websites which communicates information to web crawlers and spiders about how to scan a website.. For example, here&rsquo;s an excerpt from robots.txt for google."></head><body class=theme-base-0c><div class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><div class=sidebar-head><h1><a href=/>Jamie Magee</a></h1></div><p class=lead>Programmer, Engineer, Problem Solver</p></div><ul class=sidebar-nav><li class=sidebar-nav-item><a href=/>Home</a></li><li class=sidebar-nav-item><a href=/about>About</a></li><li class=sidebar-nav-item><a href=/blog>Archive</a></li></ul><ul class=sidebar-nav><li class=sidebar-nav-item><a href=https://github.com/JamieMagee><i class="fa fa-github-square fa-3x"></i></a>
<a href=https://www.linkedin.com/in/jamiemagee1/><i class="fa fa-linkedin-square fa-3x"></i></a>
<a href=http://twitter.com/Jamie_Magee><i class="fa fa-twitter-square fa-3x"></i></a></li></ul><p>&copy; 2023. <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC BY-NC 4.0</a></p></div></div><div class="content container"><div class=post><h1 class=post-title>A survey of robots.txt - part one</h1><span class=post-date>Sep 19, 2017 &#183; 3 minute read &#183; <a href=/blog/a-survey-of-robots-txt-part-one/#disqus_thread>Comments</a></span><p>After reading <a href=https://www.kaggle.com/colinmorris/unusual-favicons-a-brief-survey>CollinMorris&rsquo;s analysis of favicons</a> of the top 1 million sites on the web, I thought it would be interesting to do the same for other common parts of websites that often get overlooked.</p><p>The <code>robots.txt</code> file is a plain text file found at on most websites which communicates information to web crawlers and spiders about how to scan a website.. For example, here&rsquo;s an excerpt from <code>robots.txt</code> for <code>google.com</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-txt data-lang=txt><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span>User-agent: *
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span><span>Disallow: /search
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3</span><span>Allow: /search/about
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4</span><span>Allow: /search/howsearchworks
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5</span><span>...
</span></span></code></pre></div><p>The above excerpt tells all web crawlers not to scan the <code>/search</code> path, but allows them to scan <code>/search/about</code> and <code>/search/howsearchworks</code> paths. There are a few more supported keywords, but these are the most common. Following these instructions is not required, but it is considered good internet etiquette. If you want to read more about the standard, Wikipedia has a great page <a href=https://en.wikipedia.org/wiki/Robots_exclusion_standard>here</a>.</p><p>In order to do an analysis of <code>robots.txt</code>, first I need to crawl the web for them â€“ ironic, I know.</p><h2 id=scraping>Scraping</h2><p>I wrote a scraper using <a href=https://scrapy.org/>scrapy</a> to make a request for <code>robots.txt</code> for each of the domains in Alexa&rsquo;s top 1 million websites. If the response code was 200, the <code>Content-Type</code> header contained <code>text/plain</code>, and the response body was not empty, I stored the response body in a file, with the same name as the domain name.</p><p>One complication I encountered was that not all domains respond on the same protocol or subdomain. For example, some websites respond on <code>http://{domain_name}</code> while others require <code>http://www.{domain_name}</code>. If a website doesn&rsquo;t automatically redirect you to the correct protocol or subdomain, the only way to find the correct one, is to try them all! So I wrote a small class, extending scrapy&rsquo;s <code>RetryMiddleware</code>, to do this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span><span>COMMON_PREFIXES <span style=color:#000;font-weight:700>=</span> {<span style=color:#d14>&#39;http://&#39;</span>, <span style=color:#d14>&#39;https://&#39;</span>, <span style=color:#d14>&#39;http://www.&#39;</span>, <span style=color:#d14>&#39;https://www.&#39;</span>}
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span><span><span style=color:#000;font-weight:700>class</span> <span style=color:#458;font-weight:700>PrefixRetryMiddleware</span>(RetryMiddleware):
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span><span>    <span style=color:#000;font-weight:700>def</span> <span style=color:#900;font-weight:700>process_exception</span>(<span style=color:#999>self</span>, request, exception, spider):
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span><span>            prefixes_tried <span style=color:#000;font-weight:700>=</span> request<span style=color:#000;font-weight:700>.</span>meta[<span style=color:#d14>&#39;prefixes&#39;</span>]
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span><span>            <span style=color:#000;font-weight:700>if</span> COMMON_PREFIXES <span style=color:#000;font-weight:700>==</span> prefixes_tried:
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span><span>                <span style=color:#000;font-weight:700>return</span> exception
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span><span>            new_prefix <span style=color:#000;font-weight:700>=</span> choice(<span style=color:#0086b3>tuple</span>(COMMON_PREFIXES <span style=color:#000;font-weight:700>-</span> prefixes_tried))
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span><span>            request <span style=color:#000;font-weight:700>=</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>update_request(request, new_prefix)
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13</span><span>            <span style=color:#000;font-weight:700>return</span> <span style=color:#999>self</span><span style=color:#000;font-weight:700>.</span>_retry(request, exception, spider)
</span></span></code></pre></div><p>The rest of the scraper itself is quite simple, but you can read the full code <a href=https://github.com/JamieMagee/robots-txt>on GitHub</a>.</p><h2 id=results>Results</h2><p>Scraping the full Alexa top 1 million websites list took around 24 hours. Once it was finished, I had just under 700k robots.txt files</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span>$ find -type f | wc -l
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span><span><span style=color:#099>677686</span>
</span></span></code></pre></div><p>totalling 493MB</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span>$ du -sh
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span><span>493M    .
</span></span></code></pre></div><p>The smallest <code>robots.txt</code> was 1 byte<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, but the largest was over 5MB.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1</span><span>$ find -type f -exec du -Sh <span style=color:#000;font-weight:700>{}</span> + | sort -rh | head -n <span style=color:#099>1</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2</span><span>5.6M    ./haberborsa.com.tr
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4</span><span>$ find -not -empty -type f -exec du -b <span style=color:#000;font-weight:700>{}</span> + | sort -h | head -n <span style=color:#099>1</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">5</span><span><span style=color:#099>1</span>    ./0434.cc
</span></span></code></pre></div><p>The full data set is released under the <a href=https://opendatacommons.org/licenses/odbl/1.0/>Open Database License (ODbL) v1.0</a> and can be found <a href=https://github.com/JamieMagee/robots-txt>on GitHub</a></p><h2 id=whats-next>What&rsquo;s next?</h2><p>In the next part of this blog series I&rsquo;m going to analyse all the <code>robots.txt</code> to see if I can find anything interesting. In particular I&rsquo;d like to know why exactly someone needs a <code>robots.txt</code> file over 5MB in size, what is the most common web crawler listed (either allowed or disallowed), and are there any sites practising security by obscurity by trying to keep links out of search engines!</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>I needed to include <code>-not -empty</code> when looking for the smallest file, as there were errors when decoding the response body for some domains. I&rsquo;ve included the empty files in the dataset for posterity, but I will exclude them from further analysis.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div id=disqus_thread></div></div><script type=text/javascript>var disqus_shortname="jamiemagee";(function(){var e=document.createElement("script");e.async=!0,e.type="text/javascript",e.src="//"+disqus_shortname+".disqus.com/count.js",(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(e)})()</script><script type=text/javascript>var disqus_shortname="jamiemagee";(function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=http://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></body></html>