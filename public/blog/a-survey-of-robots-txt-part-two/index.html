<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">


  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>A survey of robots.txt - part two &middot; Jamie Magee</title>


  <link rel="stylesheet" href="/css/poole.css">
  <link rel="stylesheet" href="/css/hyde.css">
  <link rel="stylesheet" href="/css/poole-overrides.css">
  <link rel="stylesheet" href="/css/hyde-overrides.css">
  <link rel="stylesheet" href="/css/hyde-x.css">
  <link rel="stylesheet" href="/css/highlight/github.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/jamie.css">


  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/touch-icon-144-precomposed.png">
  <link href="/favicon.ico" rel="icon">






  <meta name="description" content="">
  <meta name="keywords" content="">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-701081-9', 'auto');

	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  <script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>
</head>
<body class="theme-base-0c">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <div class="sidebar-head">

        <img src="https://www.gravatar.com/avatar/5a37e2531db08529192d7d323e8cecd8?s=200"
             alt="gravatar" title="Jamie Magee">

      <h1><a href="/">Jamie Magee</a></h1>
      </div>
      <p class="lead">Programmer, Engineer, Problem Solver</p>
    </div>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item"><a href="/">Home</a></li>

      <li class="sidebar-nav-item "><a href="/about">About</a></li>

      <li class="sidebar-nav-item "><a href="/blog">Archive</a></li>

    </ul>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <a href="https://github.com/JamieMagee"><i class="fa fa-github-square fa-3x"></i></a>


      <a href="https://www.linkedin.com/in/jamiemagee1/"><i class="fa fa-linkedin-square fa-3x"></i></a>


      <a href="http://twitter.com/Jamie_Magee"><i class="fa fa-twitter-square fa-3x"></i></a>


      </li>
    </ul>



    <p>
  &copy; 2019. <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>
</p>

  </div>
</div>


<div class="content container">
  <div class="post">
    <h1 class="post-title">A survey of robots.txt - part two</h1>
    <span class="post-date">Mar 22, 2018 &middot; 6 minute read &middot; <a href="/blog/a-survey-of-robots-txt-part-two/#disqus_thread">Comments</a>
    </span>


<p>In <a href="/posts/a-survey-of-robots-txt-part-one/">part one</a> of this article, I collected <code>robots.txt</code> from the top 1 million sites on the web. In this article I&rsquo;m going to do some analysis, and see if there&rsquo;s anything interesting to find from all the files I&rsquo;ve collected.</p>

<p>First we&rsquo;ll start with some setup.</p>

<pre><code class="language-python">%matplotlib inline

import pandas as pd
import numpy as np
import glob
import os
import matplotlib
</code></pre>

<p>Next I&rsquo;m going to load the content of each file into my pandas dataframe, calculate the file size, and store that for later.</p>

<pre><code class="language-python">l = [filename.split('/')[1] for filename in glob.glob('robots-txt/\*')]
df = pd.DataFrame(l, columns=['domain'])
df['content'] = df.apply(lambda x: open('robots-txt/' + x['domain']).read(), axis=1)
df['size'] = df.apply(lambda x: os.path.getsize('robots-txt/' + x['domain']), axis=1)
df.sample(5)
</code></pre>

<div style="overflow-x: auto">
<table border="1">
  <thead>
    <tr>
      <th></th>
      <th>domain</th>
      <th>content</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>612419</th>
      <td>veapple.com</td>
      <td>User-agent: *\nAllow: /\n\nSitemap: http://www...</td>
      <td>260</td>
    </tr>
    <tr>
      <th>622296</th>
      <td>buscadortransportes.com</td>
      <td>User-agent: *\nDisallow: /out/</td>
      <td>29</td>
    </tr>
    <tr>
      <th>147795</th>
      <td>dailynews360.com</td>
      <td>User-agent: *\nAllow: /\n\nDisallow: /search/\...</td>
      <td>248</td>
    </tr>
    <tr>
      <th>72823</th>
      <td>newfoundlandpower.com</td>
      <td>User-agent: *\nDisallow: /Search.aspx\nDisallo...</td>
      <td>528</td>
    </tr>
    <tr>
      <th>601408</th>
      <td>xfwed.com</td>
      <td>#\n# robots.txt for www.xfwed.com\n# Version 3...</td>
      <td>201</td>
    </tr>
  </tbody>
</table>
</div>

<h1 id="file-sizes">File sizes</h1>

<p>Now that we&rsquo;ve done the setup, let&rsquo;s see what the spread of file sizes in <code>robots.txt</code> is.</p>

<pre><code class="language-python">fig = df.plot.hist(title='robots.txt file size', bins=20)
fig.set_yscale('log')
</code></pre>

<p><img src="/img/analysis_5_0.png" alt="png" /></p>

<p>It looks like the majority of <code>robots.txt</code> are under 250KB in size. This is really no surprise as <code>robots.txt</code> supports regex, so complex rulesets can be built easily.</p>

<p>Let&rsquo;s take a look at the files larger than 1MB. I can think of three possibilities: they&rsquo;re automatically maintained; they&rsquo;re some other file masquerading as <code>robots.txt</code>; or the site is doing something seriously wrong.</p>

<pre><code class="language-python">large = df[df['size'] &gt; 10 ** 6].sort_values(by='size', ascending=False)
</code></pre>

<pre><code class="language-python">import re

def count_directives(value, domain):
content = domain['content']
return len(re.findall(value, content, re.IGNORECASE))

large['disallow'] = large.apply(lambda x: count_directives('Disallow', x), axis=1)
large['user-agent'] = large.apply(lambda x: count_directives('User-agent', x), axis=1)
large['comments'] = large.apply(lambda x: count_directives('#', x), axis=1)

# The directives below are non-standard

large['crawl-delay'] = large.apply(lambda x: count_directives('Crawl-delay', x), axis=1)
large['allow'] = large.apply(lambda x: count_directives('Allow', x), axis=1)
large['sitemap'] = large.apply(lambda x: count_directives('Sitemap', x), axis=1)
large['host'] = large.apply(lambda x: count_directives('Host', x), axis=1)

large
</code></pre>

<div style="overflow-x: auto">
<table>
  <thead>
    <tr>
      <th></th>
      <th>domain</th>
      <th>content</th>
      <th>size</th>
      <th>disallow</th>
      <th>user-agent</th>
      <th>comments</th>
      <th>crawl-delay</th>
      <th>allow</th>
      <th>sitemap</th>
      <th>host</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>632170</th>
      <td>haberborsa.com.tr</td>
      <td>User-agent: *\nAllow: /\n\nDisallow: /?ref=\nD...</td>
      <td>5820350</td>
      <td>71244</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>71245</td>
      <td>5</td>
      <td>10</td>
    </tr>
    <tr>
      <th>23216</th>
      <td>miradavetiye.com</td>
      <td>Sitemap: https://www.miradavetiye.com/sitemap_...</td>
      <td>5028384</td>
      <td>47026</td>
      <td>7</td>
      <td>0</td>
      <td>0</td>
      <td>47026</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>282904</th>
      <td>americanrvcompany.com</td>
      <td>Sitemap: http://www.americanrvcompany.com/site...</td>
      <td>4904266</td>
      <td>56846</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>56852</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>446326</th>
      <td>exibart.com</td>
      <td>User-Agent: *\nAllow: /\nDisallow: /notizia.as...</td>
      <td>3275088</td>
      <td>61403</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>61404</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>579263</th>
      <td>sinospectroscopy.org.cn</td>
      <td>http://www.sinospectroscopy.org.cn/readnews.ph...</td>
      <td>2979133</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>55309</th>
      <td>vibralia.com</td>
      <td># robots.txt automaticaly generated by PrestaS...</td>
      <td>2835552</td>
      <td>39712</td>
      <td>1</td>
      <td>15</td>
      <td>0</td>
      <td>39736</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>124850</th>
      <td>oftalmolog30.ru</td>
      <td>User-Agent: *\nHost: chuzmsch.ru\nSitemap: htt...</td>
      <td>2831975</td>
      <td>87752</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>87752</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>557116</th>
      <td>la-viephoto.com</td>
      <td>User-Agent:*\nDisallow:/aloha_blog/\nDisallow:...</td>
      <td>2768134</td>
      <td>29782</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>29782</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>677400</th>
      <td>bigclozet.com</td>
      <td>User-agent: *\nDisallow: /item/\n\nUser-agent:...</td>
      <td>2708717</td>
      <td>51221</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>51221</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>621834</th>
      <td>tranzilla.ru</td>
      <td>Host: tranzilla.ru\nSitemap: http://tranzilla....</td>
      <td>2133091</td>
      <td>27647</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>27648</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>428735</th>
      <td>autobaraholka.com</td>
      <td>User-Agent: *\nDisallow: /registration/\nDisal...</td>
      <td>1756983</td>
      <td>39330</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>39330</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>628591</th>
      <td>megasmokers.ru</td>
      <td>User-agent: *\nDisallow: /*route=account/\nDis...</td>
      <td>1633963</td>
      <td>92</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>92</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>647336</th>
      <td>valencia-cityguide.com</td>
      <td># If the Joomla site is installed within a fol...</td>
      <td>1559086</td>
      <td>17719</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>17719</td>
      <td>1</td>
      <td>99</td>
    </tr>
    <tr>
      <th>663372</th>
      <td>vetality.fr</td>
      <td># robots.txt automaticaly generated by PrestaS...</td>
      <td>1536758</td>
      <td>27737</td>
      <td>1</td>
      <td>12</td>
      <td>0</td>
      <td>27737</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>105735</th>
      <td>golden-bee.ru</td>
      <td>User-agent: Yandex\nDisallow: /*_openstat\nDis...</td>
      <td>1139308</td>
      <td>24081</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
      <td>24081</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>454311</th>
      <td>dreamitalive.com</td>
      <td>user-agent: google\ndisallow: /memberprofileda...</td>
      <td>1116416</td>
      <td>34392</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>34401</td>
      <td>0</td>
      <td>9</td>
    </tr>
    <tr>
      <th>245895</th>
      <td>gobankingrates.com</td>
      <td>User-agent: *\nDisallow: /wp-admin/\nAllow: /w...</td>
      <td>1018109</td>
      <td>7362</td>
      <td>28</td>
      <td>20</td>
      <td>2</td>
      <td>7363</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<p>It looks like all of these sites are misusing <code>Disallow</code> and <code>Allow</code>. In fact, looking at the raw files it appears as if they list all of the articles on the site under an individual <code>Disallow</code> command. I can only guess that when publishing an article, a corresponding line in <code>robots.txt</code> is added.</p>

<p>Now let&rsquo;s take a look at the smallest <code>robots.txt</code></p>

<pre><code class="language-python">small = df[df['size'] &gt; 0].sort_values(by='size', ascending=True)

small.head(5)
</code></pre>

<div>
<table>
  <thead>
    <tr>
      <th></th>
      <th>domain</th>
      <th>content</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>336828</th>
      <td>iforce2d.net</td>
      <td>\n</td>
      <td>1</td>
    </tr>
    <tr>
      <th>55335</th>
      <td>togetherabroad.nl</td>
      <td>\n</td>
      <td>1</td>
    </tr>
    <tr>
      <th>471397</th>
      <td>behchat.ir</td>
      <td>\n</td>
      <td>1</td>
    </tr>
    <tr>
      <th>257727</th>
      <td>docteurtamalou.fr</td>
      <td></td>
      <td>1</td>
    </tr>
    <tr>
      <th>669247</th>
      <td>lastminute-cottages.co.uk</td>
      <td>\n</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>There&rsquo;s not really anything interesting here, so let&rsquo;s take a look at some larger files</p>

<pre><code class="language-python">small = df[df['size'] &gt; 10].sort_values(by='size', ascending=True)

small.head(5)
</code></pre>

<div>
<table>
  <thead>
    <tr>
      <th></th>
      <th>domain</th>
      <th>content</th>
      <th>size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>676951</th>
      <td>fortisbc.com</td>
      <td>sitemap.xml</td>
      <td>11</td>
    </tr>
    <tr>
      <th>369859</th>
      <td>aurora.com.cn</td>
      <td>User-agent:</td>
      <td>11</td>
    </tr>
    <tr>
      <th>329775</th>
      <td>klue.kr</td>
      <td>Disallow: /</td>
      <td>11</td>
    </tr>
    <tr>
      <th>390064</th>
      <td>chneic.sh.cn</td>
      <td>Disallow: /</td>
      <td>11</td>
    </tr>
    <tr>
      <th>355604</th>
      <td>hpi-mdf.com</td>
      <td>Disallow: /</td>
      <td>11</td>
    </tr>
  </tbody>
</table>
</div>

<p><code>Disallow: /</code> tells all webcrawlers not to crawl anything on this site, and should (hopefully) keep it out of any search engines, but not all webcrawlers follow <code>robots.txt</code>.</p>

<h1 id="user-agents">User agents</h1>

<p>User agents can be listed in <code>robots.txt</code> to either <code>Allow</code> or <code>Disallow</code> certain paths. Let&rsquo;s take a look at the most common webcrawlers.</p>

<pre><code class="language-python">from collections import Counter

def find_user_agents(content):
    return re.findall('User-agent:? (.*)', content)

user_agent_list = [find_user_agents(x) for x in df['content']]
user_agent_count = Counter(x.strip() for xs in user_agent_list for x in set(xs))
user_agent_count.most_common(n=10)
</code></pre>

<pre><code class="language-python">[('*', 587729),
('Mediapartners-Google', 36654),
('Yandex', 29065),
('Googlebot', 25932),
('MJ12bot', 22250),
('Googlebot-Image', 16680),
('Baiduspider', 13646),
('ia_archiver', 13592),
('Nutch', 11204),
('AhrefsBot', 11108)]
</code></pre>

<p>It&rsquo;s no surprise that the top result is a wildcard (<code>*</code>). Google takes spots 2, 4, and 6 with their AdSense, search and image web crawlers respectively. It does seem a little strange to see the AdSense bot listed above the usual search web crawler. Some of the other large search engines&rsquo; bots are also found in the top 10: Yandex, Baidu, and Yahoo (<code>Slurp</code>). <code>MJ12bot</code> is a crawler I had not heard of before, but according to <a href="http://mj12bot.com/">their site</a> it belongs to a UK based SEO company—and according to some of the results about it, it doesn&rsquo;t behave very well. <code>ia_archiver</code> belongs to <a href="https://archive.org/">The Internet Archive</a>, and (I assume) crawls pages for the <a href="https://archive.org/web/">Wayback Machine</a>. Finally there is <a href="https://nutch.apache.org/bot.html">Apache Nutch</a>, an open source webcrawler that can be run by anyone.</p>

<h1 id="security-by-obscurity">Security by obscurity</h1>

<p>There are certain paths that you might not want a webcrawler to know about. For example, a <code>.git</code> directory, <code>htpasswd</code> files, or parts of a site that are still in testing, and aren&rsquo;t meant to be found by anyone on Google. Let&rsquo;s see if there&rsquo;s anything interesting.</p>

<pre><code class="language-python">sec_obs = ['\.git', 'alpha', 'beta', 'secret', 'htpasswd', 'install\.php', 'setup\.php']
sec_obs_regex = re.compile('|'.join(sec_obs))

def find_security_by_obscurity(content):
return sec_obs_regex.findall(content)

sec_obs_list = [find_security_by_obscurity(x) for x in df['content']]
sec_obs_count = Counter(x.strip() for xs in sec_obs_list for x in set(xs))
sec_obs_count.most_common(10)
</code></pre>

<pre><code class="language-python">[('install.php', 28925),
('beta', 2834),
('secret', 753),
('alpha', 597),
('.git', 436),
('setup.php', 73),
('htpasswd', 45)]
</code></pre>

<p>Just because a file or directory is mentioned in <code>robots.txt</code>, it doesn&rsquo;t mean that it can actually be accessed. However, if even 1% of Wordpress installs leave their <code>install.php</code> open to the world, that&rsquo;s still a lot of vulnerable sites. Any attacker could get the keys to the kingdom very easily. The same goes for a <code>.git</code> directory. Even if it is read-only, people accidentally commit secrets to their git repository all the time.</p>

<h1 id="conclusion">Conclusion</h1>

<p><code>robots.txt</code> is a fairly innocuous part of the web. It&rsquo;s been interesting to see how popular websites (ab)use it, and which web crawlers are naughty or nice. Most of all this has been a great exercise for myself in collecting data and analysing it using pandas and Jupyter.</p>

<p>The full data set is released under the <a href="https://opendatacommons.org/licenses/odbl/1.0/">Open Database License (ODbL) v1.0</a> and can be found <a href="https://github.com/JamieMagee/robots-txt">on GitHub</a></p>

  </div>
  <div id="disqus_thread"></div>
</div>


<script type="text/javascript">
var disqus_shortname = "jamiemagee";
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>



<script type="text/javascript">
    var disqus_shortname = "jamiemagee";
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="/js/cms.js"></script>
</body>
</html>
